\section{Fairness}
Deep value iteration allows to compute $\bm{\theta}$, and hence $V(q_{t};\bm{\theta})$, for a given state $q_t$. As defined in Equation~\ref{Eq:Policy}, the policy can be defined using $V$. For this, as we are dealing with real numbers, the same trace would be selected by the policy all the time. As such, other correct traces will not be reachable in the obtained system. To remedy this, we define a fair policy that is allowed to deviate from the optimal policy with a degree of fairness.  
\begin{equation}
\label{Eq:Policy}
\pi(q) = \{a \mid q \goesto[a] q' \wedge V(q';\bm{\theta}) \geq \mathtt{max} - \mathtt{fair}\},
\end{equation}
where, $\mathtt{max} = \mathtt{maximum}\{V(q'') \mid q \goesto[a] q''\}$.  $\mathtt{fair}$ is the tolerance against the optimal policy. The value of $\mathtt{fair}$ depends on the values of the good and bad rewards and the  horizon used in deep value iteration algorithm. Clearly, the more fairness the the more deviation from the optimal policy we get. 
%In other words, when $\mathtt{fair} \maps +\infty$ 




